{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNXZLC9Ve2PE",
    "outputId": "c119ebf6-e540-4d42-83e4-cd2d2148a3f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 30 12:32:38 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 556.12                 Driver Version: 556.12         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce MX450         WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   60C    P8             N/A / ERR!  |     182MiB /   2048MiB |     11%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     10784    C+G   ...Brave-Browser\\Application\\brave.exe      N/A      |\n",
      "|    0   N/A  N/A     19056    C+G   ...Brave-Browser\\Application\\brave.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CMAKE_ARGS\"] = \"-DLLAMA_CUBLAS=on\"\n",
    "os.environ[\"FORCE_CMAKE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-rsn0DIfUGD",
    "outputId": "4aa1c89c-6103-460b-f9e2-3d7364fe7a95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
      "Collecting llama-cpp-python==0.2.77\n",
      "  Downloading llama_cpp_python-0.2.77.tar.gz (50.2 MB)\n",
      "     ---------------------------------------- 50.2/50.2 MB 6.4 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting typing-extensions>=4.5.0\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting jinja2>=2.11.3\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "     -------------------------------------- 133.3/133.3 kB 7.7 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.20.0\n",
      "  Downloading numpy-1.24.4-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "     ---------------------------------------- 14.9/14.9 MB 6.6 MB/s eta 0:00:00\n",
      "Collecting diskcache>=5.6.1\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.5/45.5 kB ? eta 0:00:00\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.5-cp38-cp38-win_amd64.whl (17 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\abhishek\\anaconda3\\envs\\env3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\abhishek\\anaconda3\\envs\\env3\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [50 lines of output]\n",
      "  *** scikit-build-core 0.10.7 using CMake 3.31.1 (wheel)\n",
      "  *** Configuring CMake...\n",
      "  2024-11-30 12:36:17,384 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
      "  loading initial cache file C:\\Users\\Abhishek\\AppData\\Local\\Temp\\tmpviukx94n\\build\\CMakeInit.txt\n",
      "  -- Building for: Visual Studio 17 2022\n",
      "  -- Selecting Windows SDK version 10.0.22000.0 to target Windows 10.0.22631.\n",
      "  -- The C compiler identification is MSVC 19.36.32537.0\n",
      "  -- The CXX compiler identification is MSVC 19.36.32537.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2022/BuildTools/VC/Tools/MSVC/14.36.32532/bin/Hostx64/x64/cl.exe - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2022/BuildTools/VC/Tools/MSVC/14.36.32532/bin/Hostx64/x64/cl.exe - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Found Git: C:/Users/Abhishek/anaconda3/envs/env3/Library/bin/git.exe (found version \"2.45.2.windows.1\")\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
      "  -- Looking for pthread_create in pthreads\n",
      "  -- Looking for pthread_create in pthreads - not found\n",
      "  -- Looking for pthread_create in pthread\n",
      "  -- Looking for pthread_create in pthread - not found\n",
      "  -- Found Threads: TRUE\n",
      "  -- Found OpenMP_C: -openmp (found version \"2.0\")\n",
      "  -- Found OpenMP_CXX: -openmp (found version \"2.0\")\n",
      "  -- Found OpenMP: TRUE (found version \"2.0\")\n",
      "  -- OpenMP found\n",
      "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:401 (message):\n",
      "    LLAMA_CUBLAS is deprecated and will be removed in the future.\n",
      "  \n",
      "    Use LLAMA_CUDA instead\n",
      "  \n",
      "  \n",
      "  -- Found CUDAToolkit: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.8/include (found version \"11.8.89\")\n",
      "  -- CUDA found\n",
      "  CMake Error at C:/Users/Abhishek/AppData/Local/Temp/pip-build-env-az226n1e/normal/Lib/site-packages/cmake/data/share/cmake-3.31/Modules/CMakeDetermineCompilerId.cmake:614 (message):\n",
      "    No CUDA toolset found.\n",
      "  Call Stack (most recent call first):\n",
      "    C:/Users/Abhishek/AppData/Local/Temp/pip-build-env-az226n1e/normal/Lib/site-packages/cmake/data/share/cmake-3.31/Modules/CMakeDetermineCompilerId.cmake:8 (CMAKE_DETERMINE_COMPILER_ID_BUILD)\n",
      "    C:/Users/Abhishek/AppData/Local/Temp/pip-build-env-az226n1e/normal/Lib/site-packages/cmake/data/share/cmake-3.31/Modules/CMakeDetermineCompilerId.cmake:53 (__determine_compiler_id_test)\n",
      "    C:/Users/Abhishek/AppData/Local/Temp/pip-build-env-az226n1e/normal/Lib/site-packages/cmake/data/share/cmake-3.31/Modules/CMakeDetermineCUDACompiler.cmake:131 (CMAKE_DETERMINE_COMPILER_ID)\n",
      "    vendor/llama.cpp/CMakeLists.txt:412 (enable_language)\n",
      "  \n",
      "  \n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  \n",
      "  *** CMake configuration failed\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\abhishek\\anaconda3\\envs\\env3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\abhishek\\anaconda3\\envs\\env3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\abhishek\\anaconda3\\envs\\env3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir llama-cpp-python==0.2.77 \\\n",
    "--force-reinstall --upgrade --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9WrC5GjbhNFx"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Importing the Llama class from the llama_cpp module\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Importing the library for data manipulation\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'"
     ]
    }
   ],
   "source": [
    "# Function to download the model from the Hugging Face model hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Importing the Llama class from the llama_cpp module\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Importing the library for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm # For progress bar related functionalities\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "06d8ebc1634a46528ae72334b18f0064",
      "cd29f1b889374f1faf53a055a1717084",
      "63a8bc343f6b477f9d09efe95c602e05",
      "2985c65821c345548eec5932d34e5397",
      "dc0f3b1c0b0c49c69c6d7c830263d131",
      "66791deae49c4024b812e3c1b8f582e5",
      "dc3d940a627740e19e6dbfd1edb5168b",
      "ad41540cd539475fbffd0d13e2240763",
      "d251b820069a426baba8fda9ff5bf67a",
      "ee2c706f497c4cca95646c8cb01ce34a",
      "355c8cc0ad6f458faf065e88ed12bbb9"
     ]
    },
    "id": "Kji0g4TZhbUH",
    "outputId": "060f3080-4fe9-47a1-dbe9-a6b5643f6f13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d8ebc1634a46528ae72334b18f0064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama-2-13b-chat.Q5_K_M.gguf:   0%|          | 0.00/9.23G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Model configuration\n",
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
    "model_basename = \"llama-2-13b-chat.Q5_K_M.gguf\"\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=model_name_or_path,\n",
    "    filename=model_basename\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMt37VFEiIag",
    "outputId": "d074c9b7-7d8f-4b35-943a-411961fc17a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/llama-2-13b-chat.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q5_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 8.60 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.37 MiB\n",
      "llm_load_tensors: offloading 10 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 10/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  8801.63 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  2166.07 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 6016\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =  3525.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1175.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4700.00 MiB, K (f16): 2350.00 MiB, V (f16): 2350.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   636.44 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    31.76 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 334\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2,  # CPU cores\n",
    "    n_batch=512,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=10,  # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_ctx=6000,  # Context window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CeMz_rdXiX0Z"
   },
   "outputs": [],
   "source": [
    "# function to generate, process, and return the response from the LLM\n",
    "def generate_llama_response(instruction, user_prompt):\n",
    "\n",
    "    # System message\n",
    "    system_message = f\"\"\"\n",
    "    [INST]<<SYS>>\n",
    "    {instruction}\n",
    "    <</SYS>>[/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine user_prompt and system_message to create the prompt\n",
    "    prompt = f\"{user_prompt}\\n{system_message}\"\n",
    "\n",
    "    # Generate a response from the LLaMA model\n",
    "    response = lcpp_llm(\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        temperature=0.01,\n",
    "        top_p=0.95,\n",
    "        repeat_penalty=1.2,\n",
    "        top_k=50,\n",
    "        stop=['INST'],\n",
    "        echo=False\n",
    "    )\n",
    "\n",
    "    # Extract and return the response text\n",
    "    response_text = response[\"choices\"][0][\"text\"]\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtodE1yGich-"
   },
   "outputs": [],
   "source": [
    "instruction_1 = \"\"\"\n",
    "    You are an AI analyzing restaurant reviews. Classify the sentiment of the provided review into the following categories:\n",
    "    - Positive\n",
    "    - Negative\n",
    "    - Neutral\n",
    "\n",
    "    Format the output as a JSON object with a single key-value pair as shown below:\n",
    "    {\"sentiment\": \"your_sentiment_prediction\"}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUzkB9qojYUV"
   },
   "outputs": [],
   "source": [
    "review =\"\"\"\n",
    "This film is one of the all-time classics -- it won Academy Awards for Best Screenplay, Best Cinematography, Best Musical Score, and Best Original Song; it was nominated for Best Sound, Best Director, and Best Picture.  This movie is about as well put together as a movie can get. Legendary screenwriter, William Goldman, gifts the characters with sharp, witty, quotable dialogue which Paul Newman and Robert Redford deliver with deadpan perfection.  What little time the supporting cast gets, they use well (with a particular emphasis on Strother Martin's unforgettably \"colorful\" performance as Percy Garris.  Burt Bacharach's 60s-vibe score sounds a little odd for a western, but it offers a tone that period music just wouldn't deliver. The real gem of this film, though, is Conrad Hall's photography.  Intricately and beautifully crafted, the images are the main storytelling vocabulary.  There is heart, metaphor, character, and action in every frame of this masterpiece.  Film buffs and other careful viewers are treated to subtle nuances in lighting, angle, framing, and composition that enhance each line of dialogue, each action sequence, and the theme as a whole.  Stunning.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "uO31PdPekEsk",
    "outputId": "249eaa7a-17a4-4536-f486-24889d92c0ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   35964.77 ms\n",
      "llama_print_timings:      sample time =     239.66 ms /   357 runs   (    0.67 ms per token,  1489.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   35963.72 ms /   404 tokens (   89.02 ms per token,    11.23 tokens per second)\n",
      "llama_print_timings:        eval time =  367376.21 ms /   356 runs   ( 1031.96 ms per token,     0.97 tokens per second)\n",
      "llama_print_timings:       total time =  404205.19 ms /   760 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Sure, I\\'d be happy to help! Here is the review you provided:\\n\\n\"This film is one of the all-time classics -- it won Academy Awards for Best Screenplay, Best Cinematography, Best Musical Score, and Best Original Song; it was nominated for Best Sound, Best Director, and Best Picture.  This movie is about as well put together as a movie can get. Legendary screenwriter, William Goldman, gifts the characters with sharp, witty, quotable dialogue which Paul Newman and Robert Redford deliver with deadpan perfection.  What little time the supporting cast gets, they use well (with a particular emphasis on Strother Martin\\'s unforgettably \"colorful\" performance as Percy Garris.  Burt Bacharach\\'s 60s-vibe score sounds a little odd for a western, but it offers a tone that period music just wouldn\\'t deliver. The real gem of this film, though, is Conrad Hall\\'s photography.  Intricately and beautifully crafted, the images are the main storytelling vocabulary.  There is heart, metaphor, character, and action in every frame of this masterpiece.  Film buffs and other careful viewers are treated to subtle nuances in lighting, angle, framing, and composition that enhance each line of dialogue, each action sequence, and the theme as a whole.  Stunning.\"\\n\\nBased on the review, I would classify the sentiment as Positive. Here is the output in JSON format:\\n\\n{\"sentiment\": \"Positive\"}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_llama_response(instruction_1, review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KBBXfGLxfAo"
   },
   "outputs": [],
   "source": [
    "# defining a function to parse the JSON output from the model\n",
    "def extract_json_data(json_str):\n",
    "    import json\n",
    "    try:\n",
    "        # Find the indices of the opening and closing curly braces\n",
    "        json_start = json_str.find('{')\n",
    "        json_end = json_str.rfind('}')\n",
    "\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            extracted_category = json_str[json_start:json_end + 1]  # Extract the JSON object\n",
    "            data_dict = json.loads(extracted_category)\n",
    "            return data_dict\n",
    "        else:\n",
    "            print(f\"Warning: JSON object not found in response: {json_str}\")\n",
    "            return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lb_mOl5xl7Vv"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('stock_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ce_Ch3tPmD5M"
   },
   "outputs": [],
   "source": [
    "data[\"Date\"] = pd.to_datetime(data['Date'])  # Convert the 'Date' column to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTMGN1JpmDrY"
   },
   "outputs": [],
   "source": [
    "# Group the data by week using the 'Date' column.\n",
    "weekly_grouped = data.groupby(pd.Grouper(key='Date', freq='W'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQGyC4WWmRB9",
    "outputId": "dac88ab9-625c-45fb-e14a-9cdfaff5fc99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 2)\n"
     ]
    }
   ],
   "source": [
    "weekly_grouped = weekly_grouped.agg(\n",
    "    {\n",
    "        'News': lambda x: ' || '.join(x)  # Join the news values with ' || ' separator.\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "print(weekly_grouped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoW6kRGYmiEp"
   },
   "outputs": [],
   "source": [
    "# creating a copy of the data\n",
    "data_1 = weekly_grouped.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkS9LQRqkLt8"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an expert data analyst specializing in news content analysis.\n",
    "Analyse the provided news headlines carefully and the return the top-3 positive events and top-3 negative events that impact the stock prices.\n",
    "\n",
    "Format the output as a JSON object with two key-value pairs as shown below:\n",
    "{\"Positive Events\": \"[your top-3 positive events prediction]\", \"Negative Events\": \"[your top-3 positive events prediction]\"}\n",
    "\"\"\"\n",
    "\n",
    "news_text = data_1['News'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-E_fdC-xM0m"
   },
   "outputs": [],
   "source": [
    "news_text1 = \" || \".join(news_text.split(\" || \")[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zsas8USanNeK",
    "outputId": "a3b744a2-cc1b-4690-d96b-3ff3d685b20d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "llm_response = generate_llama_response(prompt, news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SYmCdy6oDB7"
   },
   "outputs": [],
   "source": [
    "extract_json_data(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "la41830Vx5K0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06d8ebc1634a46528ae72334b18f0064": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cd29f1b889374f1faf53a055a1717084",
       "IPY_MODEL_63a8bc343f6b477f9d09efe95c602e05",
       "IPY_MODEL_2985c65821c345548eec5932d34e5397"
      ],
      "layout": "IPY_MODEL_dc0f3b1c0b0c49c69c6d7c830263d131"
     }
    },
    "2985c65821c345548eec5932d34e5397": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee2c706f497c4cca95646c8cb01ce34a",
      "placeholder": "​",
      "style": "IPY_MODEL_355c8cc0ad6f458faf065e88ed12bbb9",
      "value": " 9.23G/9.23G [01:57&lt;00:00, 83.9MB/s]"
     }
    },
    "355c8cc0ad6f458faf065e88ed12bbb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "63a8bc343f6b477f9d09efe95c602e05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad41540cd539475fbffd0d13e2240763",
      "max": 9229924224,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d251b820069a426baba8fda9ff5bf67a",
      "value": 9229924224
     }
    },
    "66791deae49c4024b812e3c1b8f582e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad41540cd539475fbffd0d13e2240763": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd29f1b889374f1faf53a055a1717084": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66791deae49c4024b812e3c1b8f582e5",
      "placeholder": "​",
      "style": "IPY_MODEL_dc3d940a627740e19e6dbfd1edb5168b",
      "value": "llama-2-13b-chat.Q5_K_M.gguf: 100%"
     }
    },
    "d251b820069a426baba8fda9ff5bf67a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dc0f3b1c0b0c49c69c6d7c830263d131": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc3d940a627740e19e6dbfd1edb5168b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee2c706f497c4cca95646c8cb01ce34a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
